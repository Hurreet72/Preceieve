# Preceieve 
A web-based application that converts speech to corresponding sign language animations, enabling better communication accessibility for individuals with hearing impairments.

This project uses **Python**, **Speech Recognition**, and **OpenCV / MediaPipe** along with a **web frontend** to detect spoken words and translate them into animated sign language gestures.


## ğŸš€ Features
- ğŸ¤ Real-time Speech Recognition
- âœ‹ Converts recognized speech into Sign Language gesture animations
- ğŸŒ Simple and interactive web interface
- ğŸ”´ Emergency detection alert
- ğŸ“¦ Easy to install & run locally
- ğŸ§  Built using Machine Learning and Computer Vision


## ğŸ› ï¸ Tech Stack
| Frontend | Backend / ML | Tools | HTML, CSS, JS | Python, OpenCV, TensorFlow / MediaPipe | Git, Flask, SpeechRecognition |


## ğŸ’» How to Run Locally

1ï¸âƒ£ Clone the repository: 
git clone https://github.com/Hurreet72/Preceieve.git

2ï¸âƒ£ Navigate to project directory: 
cd Preceieve

3ï¸âƒ£ Create Virtual Environment: 
python -m venv venv

4ï¸âƒ£ Activate Virtual Environment
For Windows (PowerShell): 
venv\Scripts\activate
For Mac/Linux: 
source venv/bin/activate

5ï¸âƒ£ Install Dependencies: 
pip install -r requirements.txt

6ï¸âƒ£ Run the Project: 
python starter.py


## ğŸ”® Future Scope

- Real-time 3D animated sign language models.

- Support for multiple regional and global sign languages (ASL, BSL, ISL).

- Convert into a mobile app using Flutter / React Native.

- Emotion and facial expression recognition for improved context.

- Enhanced AI emergency detection using deep learning datasets.

- Direction-based sound detection (left, right, front, back).

- Integration with vibration-based wearable devices.

- Cloud-based user profiles and personalization settings.

- Sign language learning modules with quizzes and performance tracking.
